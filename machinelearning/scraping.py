import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
import os

# User-Agent pour Ã©viter le blocage
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
}

def scrape_covid_data(country, url):
    """Scrape les statistiques COVID pour un pays donnÃ© en gÃ©rant les erreurs"""
    try:
        response = requests.get(url, headers=HEADERS, timeout=10)
        response.raise_for_status()
    except requests.RequestException as e:
        print(f"âš ï¸ Erreur pour {country} ({url}) : {e}")
        return [country, "N/A", "N/A", "N/A"]

    time.sleep(1)  # Pause pour Ã©viter le blocage

    soup = BeautifulSoup(response.text, 'html.parser')

    # RÃ©cupÃ©rer les statistiques principales
    stats = soup.find_all("div", class_="maincounter-number")
    if stats and len(stats) >= 3:
        cases = stats[0].text.strip().replace(",", "")
        deaths = stats[1].text.strip().replace(",", "")
        recovered = stats[2].text.strip().replace(",", "")
        return [country, cases, deaths, recovered]
    else:
        print(f"âš ï¸ Pas assez de donnÃ©es pour {country}")
        return [country, "N/A", "N/A", "N/A"]

def scrape_all_countries():
    """Scrape les donnÃ©es COVID Ã  partir des URLs du fichier CSV"""
    try:
        df = pd.read_csv("data/country_urls.csv")
    except FileNotFoundError:
        print("âŒ Fichier 'data/country_urls.csv' introuvable. ExÃ©cute d'abord le script de gÃ©nÃ©ration d'URLs.")
        return

    # VÃ©rifier les premiÃ¨res URLs gÃ©nÃ©rÃ©es
    print("\nğŸ” Exemple d'URLs utilisÃ©es :")
    for i, row in df.iterrows():
        print(f"ğŸŒ {row['Country']} â†’ {row['URL']}")
        if i == 5:
            break

    # Liste pour stocker les donnÃ©es
    data = []
    
    print(f"\nğŸ”„ Scraping de {len(df)} pays...\n")
    for _, row in df.iterrows():
        country, url = row["Country"], row["URL"]
        print(f"ğŸ“¡ Scraping {country} ({url})...")
        result = scrape_covid_data(country, url)
        data.append(result)

    # Supprime l'ancien fichier avant d'enregistrer le nouveau
    if os.path.exists("data/covid_data_all_countries.csv"):
        os.remove("data/covid_data_all_countries.csv")

    # Sauvegarde dans un fichier CSV
    df_out = pd.DataFrame(data, columns=["Country", "Cases", "Deaths", "Recovered"])
    df_out.to_csv("data/covid_data_all_countries.csv", index=False)

    print("\nâœ… Scraping terminÃ© ! DonnÃ©es enregistrÃ©es dans data/covid_data_all_countries.csv")

# ExÃ©cuter le scraping
scrape_all_countries()

